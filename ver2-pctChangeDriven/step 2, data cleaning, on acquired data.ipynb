{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: merge all .csv files into a one-year dataframe\n",
    "\n",
    "loop through the lob_caps directory, forming one time-sorted dataframe, with all CAPS files. These files captured sample bid and ask capitalization, and respective bid and ask volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from matplotlib) (6.0.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: altair in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (5.0.1)\n",
      "Requirement already satisfied: jinja2 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (4.19.0)\n",
      "Requirement already satisfied: numpy in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.18 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (1.5.3)\n",
      "Requirement already satisfied: toolz in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from altair) (4.7.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (6.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jsonschema>=3.0->altair) (0.9.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from pandas>=0.18->altair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from pandas>=0.18->altair) (2022.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from jinja2->altair) (2.1.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/stefan/Desktop/GRUS/caret1/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install matplotlib\n",
    "!pip3 install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: missing destination file operand after 'backup_match/'\n",
      "Try 'mv --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "!mv $(find . -type d -name \"lob_caps\" -exec grep -q MATCH {} \\; -print0 | xargs -0 echo) backup_match/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for new df:  298660\n",
      "start:  1660221600292.0  end:  1693078943553.0\n",
      "Index(['bc', 'ac', 'tbv', 'tav', 'time', 'mp', 'minBid'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/a/21232849 model \n",
    "def getCAPSByDateAndType(type):  #returns a dict, date + df caps for that date, then extended date and time\n",
    "                                # print(\"for type, \", type)\n",
    "    ret = []\n",
    "    for root, dirs, files in os.walk(\"./lob_caps/\"):\n",
    "        for filename in files:\n",
    "            if type in filename:\n",
    "#                 print(\"CAPS file, \", filename) #mac, do find . -name ._\\* -delete\n",
    "                ret.append(filename)\n",
    "    return ret\n",
    "\n",
    "csvFileList = getCAPSByDateAndType(\"CAPS\") #iterate this array to dip into each csv, later on\n",
    "li = []                         #form the endFrame / global data frame around this array\n",
    "for filename in csvFileList:\n",
    "    csv = \"lob_caps/\" + filename\n",
    "    # print(csv)\n",
    "    df = pd.read_csv(csv, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "capsFrame = pd.concat(li, axis=0, ignore_index=True) #end frame contains all data\n",
    "capsFrame.sort_values(by=['time'], ascending=True)   #sorted by time into one time series\n",
    "print(\"for new df: \", capsFrame.shape[0])\n",
    "start = capsFrame[\"time\"].min()\n",
    "end = capsFrame[\"time\"].max()\n",
    "print(\"start: \", start, \" end: \", end)\n",
    "print(capsFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getSKEWByDateAndType(type):  #returns a dict, date + df caps for that date, then extended date and time\n",
    "#                                 # print(\"for type, \", type)\n",
    "#     ret = []\n",
    "#     for root, dirs, files in os.walk(\"./lob_caps/\"):\n",
    "#         for filename in files:\n",
    "#             if type in filename:\n",
    "# #                 print(\"CAPS file, \", filename) #mac, do find . -name ._\\* -delete\n",
    "#                 ret.append(filename)\n",
    "#     return ret\n",
    "\n",
    "# csvFileList = getSKEWByDateAndType(\"MEANSHIFT\") #iterate this array to dip into each csv, later on\n",
    "# li = []                         #form the endFrame / global data frame around this array\n",
    "# for filename in csvFileList:\n",
    "#     csv = \"lob_caps/\" + filename\n",
    "#     # print(csv)\n",
    "#     df = pd.read_csv(csv, index_col=None, header=0)\n",
    "#     li.append(df)\n",
    "\n",
    "# skewFrame = pd.concat(li, axis=0, ignore_index=True) #end frame contains all data\n",
    "# skewFrame.sort_values(by=['timeStamp'], ascending=True)   #sorted by time into one time series\n",
    "# skewFrame.rename(columns={'timeStamp': 'time'}, inplace=True)\n",
    "# print(\"for new df: \", skewFrame.shape[0])\n",
    "# start = skewFrame[\"time\"].min()\n",
    "# end = skewFrame[\"time\"].max()\n",
    "# print(\"start: \", start, \" end: \", end)\n",
    "# print(skewFrame.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging those two data frames Will not take place based on a Shared key of time\n",
    "# You'll need to look up the SKU value for every row based on approximation\n",
    "# merged_df = pd.merge(capsFrame, skewFrame, on='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewFrame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capsFrame.loc[capsFrame['time'].sub(skewFrame['time'].values[0]).abs().idxmin(), 'mean'] = skewFrame['mean'].values[0]\n",
    "# capsFrame.loc[capsFrame['time'].sub(skewFrame['time'].values[0]).abs().idxmin(), 'skew'] = skewFrame['skew'].values[0]\n",
    "# capsFrame.loc[capsFrame['time'].sub(skewFrame['time'].values[0]).abs().lt(pd.Timedelta(minutes=2)).idxmax(), 'mean'] = skewFrame['mean'].values[0]\n",
    "# \n",
    "# Could not get this operation to work try it again using a range of values once the precursor and surge are defined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## schema for capitalization data\n",
    "\n",
    "loads the csv files, as acquired from coinbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bc</th>\n",
       "      <th>ac</th>\n",
       "      <th>tbv</th>\n",
       "      <th>tav</th>\n",
       "      <th>time</th>\n",
       "      <th>mp</th>\n",
       "      <th>minBid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2828853.84</td>\n",
       "      <td>10260926.86</td>\n",
       "      <td>541682.81</td>\n",
       "      <td>221178.78</td>\n",
       "      <td>1.672934e+12</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2826677.43</td>\n",
       "      <td>10256711.31</td>\n",
       "      <td>541497.46</td>\n",
       "      <td>220827.12</td>\n",
       "      <td>1.672934e+12</td>\n",
       "      <td>11.76</td>\n",
       "      <td>11.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           bc           ac        tbv        tav          time     mp  minBid\n",
       "0  2828853.84  10260926.86  541682.81  221178.78  1.672934e+12  11.76   11.71\n",
       "1  2826677.43  10256711.31  541497.46  220827.12  1.672934e+12  11.76   11.71"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capsFrame.head(2) #shows the basic data collection via coinbase, these are aggregated values, collected several x a minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover precursor and surge episodes\n",
    "\n",
    "the goal of the data prep is to discover periods of continuous, positive momentum. These are **surges**. \n",
    "\n",
    "The periods preceding surges are, for the sake of the experiment, **precursors**. They are detected as periods of discontinuous positive momentum, or negative momentum. \n",
    "\n",
    "A ten-row window is used to calculate positive or negative momentum. A percent **change** is calculated for the ten row subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get percent change as basis for comprehending LOB\n",
    "\n",
    "create new columns which depict the momentum of one row versus the next, in terms of price , capitalization and volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298660 Index(['bc', 'ac', 'tbv', 'tav', 'time', 'mp', 'minBid', 'change', 'bc_change',\n",
      "       'ac_change', 'tav_change', 'tbv_change'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load your time series data into a pandas dataframe\n",
    "caps_df = capsFrame   \n",
    "lookback_period = 10 # in rows\n",
    "caps_df['change'] = caps_df['mp'].pct_change(periods=lookback_period)\n",
    "caps_df['bc_change'] = caps_df['bc'].pct_change(periods=lookback_period)\n",
    "\n",
    "caps_df['ac_change'] = caps_df['ac'].pct_change(periods=lookback_period)\n",
    "\n",
    "caps_df['tav_change'] = caps_df['tav'].pct_change(periods=lookback_period)\n",
    "\n",
    "caps_df['tbv_change'] = caps_df['tbv'].pct_change(periods=lookback_period)\n",
    "\n",
    "# caps_df.sample\n",
    "print(caps_df.shape[0], caps_df.columns)# Calculate the returns of your asset over a fixed lookback period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  establish benchmarks for percent change\n",
    "\n",
    "the mean of change represents the average rate of change between LOB samples. This is used to determine whether the change between rows is significant or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005284"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for period, average or mean change metric. this changes with window size\n",
    "meanChange = round(caps_df['change'].mean(),8)\n",
    "meanChange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define precursors from surges\n",
    "\n",
    "use the threshold, mean change as tool to separate precursor from surges, where surges represent periods of positive momentum above threshold.\n",
    "\n",
    "This step defines the data schema for the remainder of the process, where key statistics are defined for precursors and surges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify units of 10 rows where the percent change is greater or less than the threshold\n",
    "threshold = meanChange\n",
    "surges = []\n",
    "precursors = []\n",
    "for i in range(0,len(caps_df),10):\n",
    "    if caps_df.iloc[i:i+10]['change'].mean() >= threshold:\n",
    "        surges.append({'time': caps_df.iloc[i]['time'],\n",
    "                       's_MP': caps_df.iloc[i]['mp'],\n",
    "                       'change': caps_df.iloc[i:i+10]['change'].mean(),\n",
    "                       'type':'surge'})  #['bc', 'ac', 'tbv', 'tav', 'time', 'mp', 'minBid', 'change']\n",
    "    else:\n",
    "        precursors.append({'time': caps_df.iloc[i]['time'],\n",
    "                           'p_MP': caps_df.iloc[i]['mp'],\n",
    "                           'change': caps_df.iloc[i:i+10]['change'].mean(),\n",
    "                            'type':'precursor',\n",
    "                            'p_buyCap':caps_df.iloc[i]['bc'], \n",
    "                            'p_askCap':caps_df.iloc[i]['ac'],\n",
    "                            'p_totalBidVol':caps_df.iloc[i]['tbv'],\n",
    "                            'p_totalAskVol':caps_df.iloc[i]['tav']\n",
    "                            })  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in surges:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for item in precursors:\n",
    "#     print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge precursors and surges into time series\n",
    "\n",
    "a dataframe of sequences, **sequence_df** is created by concatenating both buckets, and sorting by time. This will create a time series of surge and precursor periods, as defined by: \n",
    "\n",
    "* 10 window percent change values\n",
    "* contiguity: these precursor and surges are next to each other and thus have a length or duration of momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surges_df = pd.DataFrame(surges)\n",
    "precursors_df = pd.DataFrame(precursors)\n",
    "sequence_df = pd.concat([surges_df, precursors_df]).sort_values(by=['time'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### view the aligned, continuous time series of precursors and surges\n",
    "\n",
    "view the final abstraction: sets of precursor periods, next to surges, in a linear time series. Precursors effectively precede surges on a linear time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in sequence_df.iterrows():\n",
    "#     print(row['surge'], row['precursor'])\n",
    "sequence_df['type'].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_df.head(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize proof of algorithmic accuracy\n",
    "\n",
    "this chart will plot the price time series, with an area of precursor and surge, as proof of our algorithmic accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = sequence_df[:4999]\n",
    "line = alt.Chart(subset).mark_line(color='green').encode(\n",
    "    x='time',\n",
    "    y='s_MP'\n",
    ")\n",
    "\n",
    "s_bar = alt.Chart(subset).mark_bar().encode(\n",
    "    x='time',\n",
    "    y='s_MP',\n",
    "    color='type:N'\n",
    ")\n",
    "\n",
    "p_bar = alt.Chart(subset).mark_bar().encode(\n",
    "    x='time',\n",
    "    y='p_MP',\n",
    "    color='type:N'\n",
    ")\n",
    "\n",
    "chart = (line + s_bar + p_bar).properties(width=600, height=500)\n",
    "chart.title = 'Sequential order of precursor and surges for April 7th 2023'\n",
    "subtitle = 'Precursors are contiguous periods where percentage rate of growth is less than threshold'\n",
    "chart.properties(title=alt.TitleParams(text=[chart.title, subtitle], baseline='bottom', orient='top', anchor='start', fontSize=14))\n",
    "chart.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform information gain on grouped precursors and surges\n",
    "\n",
    "define the **sum change**, or total change per continuous episode (precursor or surge). \n",
    "\n",
    "define the **length** of each episode. \n",
    "\n",
    "define the height of the surge, how high did the continuous positive momentum reach?\n",
    "\n",
    "define the size (area) of the surge, as a triangular area (height times length), as **surge_area**\n",
    "\n",
    "Create one line to describe a precursor or search and it's related order book statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_df['group'] = (sequence_df['type'] != sequence_df['type'].shift(1)).cumsum()\n",
    "sequence_df['length'] = sequence_df.groupby(['type', 'group'])['group'].transform('count')\n",
    "# sequence_df['identifier'] = sequence_df.groupby(['type', 'group'])['time'].transform('min') #prep the label early, if surge?\n",
    "print(sequence_df.shape[0])\n",
    "sequence_df['sum_change'] = sequence_df.groupby(['type', 'group'])['change'].transform('sum')\n",
    "print(sequence_df.shape[0])\n",
    "\n",
    "sequence_df['area']  = sequence_df.apply(lambda row: row['length'] * row['sum_change'], axis=1)\n",
    "sequence_df.loc[sequence_df['type'] == 'surge', 'surge_area'] = sequence_df.loc[sequence_df['type'] == 'surge', 'area']\n",
    "sequence_df.columns\n",
    "# sequence_df['area'] = sequence_df.groupby(['type', 'group'])['sum_change'].multiply(sequence_df['length'])\n",
    "# sequence_df.loc[sequence_df['type'] == 'surge', 'surge_area'] =  sequence_df['length'] * sequence_df['sum_change']\n",
    "\n",
    "\n",
    "# sequence_df['sum'] = sequence_df.groupby(['surge', 'group'])['change'].transform('sum')\n",
    "\n",
    "# sequence_df['end_time'] = sequence_df.groupby(['surge', 'group'])['time'].transform('max')\n",
    "# sequence_df['type'] = sequence_df['surge']\n",
    "\n",
    "# sequence_df['buyCapSum'] = sequence_df.groupby(['surge', 'group'])['buyCap'].transform('avg')\n",
    "# sequence_df['askCapSum'] = sequence_df.groupby(['surge', 'group'])['askCap'].transform('avg')\n",
    "\n",
    "# calculate the area for the surge\n",
    "\n",
    "# sequence_df = sequence_df.drop('next_value', axis=1)\n",
    "# sequence_df.loc[sequence_df['bucket'] == 'surge', 'surge_length'] =  sequence_df['length']\n",
    "# sequence_df.drop('length', axis=1, inplace=True)\n",
    "# df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "#unique_df = sequence_df.groupby('identifier').first().reset_index()\n",
    "\n",
    "# unique_df.loc[unique_df['surge'] == '1', 'surge_length'] = unique_df['length']\n",
    "# unique_df.loc[unique_df['surge'] == '0', 'length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critical group by unique identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_df = sequence_df.groupby('group').first().reset_index()\n",
    "print(unique_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge even and odd Rows to form the final sequences\n",
    "\n",
    "Even rows contain surge, and odd rows contain precursors. **When you merge them, you form a sequence of precursor, and surge.**\n",
    "\n",
    "Each row will contain a continuous **precursor->surge** sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_df = unique_df.iloc[::2].reset_index(drop=True)\n",
    "odd_df = unique_df.iloc[1::2].reset_index(drop=True)\n",
    "\n",
    "merged_df = pd.concat([even_df, odd_df], axis=1)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols = merged_df.dropna(axis=1, how='all')\n",
    "nan_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_cols.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to CSV: step one, pipeline\n",
    "Label to use is surge_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_cols = nan_cols.rename(columns={'group': 'group_1', 'time': 'time_1', 'change': 'change_1', 'type': 'type_1', 'length': 'length_1', 'sum_change': 'sum_change_1', 'area': 'area_1'})\n",
    "# writeable_df = nan_cols['group', 'time', 's_MP', 'change', 'type', 'length', 'sum_change','area', \\\n",
    "#                         'surge_area', 'group', 'time', 'change', 'type', 'p_MP',\n",
    "#                        'p_buyCap', 'p_askCap', 'p_totalBidVol', 'p_totalAskVol', 'length','sum_change', 'area']\n",
    "\n",
    "# writeable_df.columns = ['group', 'time', 's_MP', 'change', 'type', 'length', 'sum_change',\n",
    "#        's_area', 'surge_area', 'p_group', 'p_time', 'p_change', 'p_type', 'p_MP',\n",
    "#        'p_buyCap', 'p_askCap', 'p_totalBidVol', 'p_totalAskVol', 'p_length',\n",
    "#        'p_sum_change', 'p_area']\n",
    "nan_cols.to_csv('pipeline1.csv', index=False)\n",
    "# df.to_csv('filename.csv', index=False)\n",
    "# writeable_df.to_csv('pipeline1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The repeating elements in the list are:\n",
    "- group\n",
    "- time\n",
    "- change\n",
    "- type\n",
    "- length\n",
    "- sum_change\n",
    "- area '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
